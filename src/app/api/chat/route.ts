import { NextRequest, NextResponse } from "next/server";
import { getGroqClient } from "@/lib/groq";
import { isOllamaAvailable, ollamaStream, OLLAMA_MODEL } from "@/lib/ollama";
import { getRAGContext, buildRAGSystemPrompt, detectNavigationIntent } from "@/lib/system-prompt-rag";
import { getCachedResponse, setCachedResponse } from "@/lib/response-cache";
import { logInteraction } from "@/lib/audit-logger";
import { ChatRequest } from "@/types";
import { GROQ_MODEL } from "@/lib/constants";

export async function POST(req: NextRequest) {
  console.log("ğŸš€ [API] Chat route called");
  
  try {
    const body: ChatRequest = await req.json();
    const { message, language = "en", conversationHistory = [] } = body;
    console.log("ğŸ“ [API] Received message:", message);
    console.log("ğŸŒ [API] Language:", language);
    console.log("ğŸ’¬ [API] History length:", conversationHistory.length);

    if (!message?.trim()) {
      console.error("âŒ [API] Empty message received");
      return NextResponse.json({ error: "Message is required" }, { status: 400 });
    }

    // â”€â”€ Prompt injection guard â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const INJECTION_PATTERNS = /ignore\s+(all\s+)?previous\s+instructions|you\s+are\s+now|forget\s+(all\s+)?your\s+rules|new\s+system\s+prompt|disregard\s+(all\s+)?prior|act\s+as\s+(?:a\s+)?(?:different|general|new)|reveal\s+(?:your\s+)?(?:system|instructions)/i;
    if (INJECTION_PATTERNS.test(message)) {
      console.warn("ğŸ›¡ï¸ [API] Prompt injection attempt blocked:", message.substring(0, 80));
      const refusal = "I can only help with election procedures and poll worker training. Please contact your election supervisor for other questions.";
      const encoder = new TextEncoder();
      const stream = new ReadableStream({
        start(controller) {
          controller.enqueue(encoder.encode(`data: ${JSON.stringify({ delta: refusal })}\n\n`));
          controller.enqueue(encoder.encode(`data: ${JSON.stringify({ done: true, sources: [] })}\n\n`));
          controller.close();
        },
      });
      return new Response(stream, {
        headers: { "Content-Type": "text/event-stream", "Cache-Control": "no-cache" },
      });
    }

    // Check cache first â€” return as a single JSON chunk with cached flag
    const cached = getCachedResponse(message);
    if (cached) {
      console.log("âœ… [API] Cache hit! Returning cached response");
      logInteraction({
        userType: "poll_worker",
        question: message,
        response: cached.response,
        sourceDoc: cached.source,
        language,
        cached: true,
      });

      const encoder = new TextEncoder();
      const stream = new ReadableStream({
        start(controller) {
          controller.enqueue(
            encoder.encode(
              `data: ${JSON.stringify({ content: cached.response, source: cached.source, sourceMeta: cached.sourceMeta, cached: true, done: true })}\n\n`
            )
          );
          controller.close();
        },
      });
      return new Response(stream, {
        headers: { "Content-Type": "text/event-stream", "Cache-Control": "no-cache" },
      });
    }

    // Check for navigation intent
    const navIntent = detectNavigationIntent(message);
    if (navIntent) {
      console.log("ğŸ§­ [API] Navigation intent detected:", navIntent.path);
    }

    // Get RAG context from knowledge base
    const { context: ragContext, sourceMeta } = await getRAGContext(message);
    console.log("ğŸ“š [API] RAG context length:", ragContext.length);

    // â”€â”€ DEBUG: Log each retrieved chunk so we can verify what the LLM sees â”€â”€
    if (sourceMeta.length > 0) {
      console.log("\nğŸ“‹ [RAG] â•â•â• RETRIEVED CHUNKS â•â•â•");
      sourceMeta.forEach((m, i) => {
        console.log(`  [${i + 1}] Score: ${m.score.toFixed(3)} | Page ${m.pageNumber} | Â§${m.sectionTitle}`);
        console.log(`      Content (first 200 chars): ${m.chunkContent.substring(0, 200)}`);
      });
      console.log("ğŸ“‹ [RAG] â•â•â• END CHUNKS â•â•â•\n");
    } else {
      console.warn("âš ï¸ [RAG] No chunks retrieved for query:", message);
    }

    const systemPrompt = buildRAGSystemPrompt(language, ragContext, true);
    // â”€â”€ DEBUG: Log the full system prompt (truncated) â”€â”€
    console.log("\nğŸ§  [PROMPT] â•â•â• SYSTEM PROMPT (first 1500 chars) â•â•â•");
    console.log(systemPrompt.substring(0, 1500));
    console.log("ğŸ§  [PROMPT] â•â•â• END â•â•â•\n");
    const chatMessages = [
      { role: "system" as const, content: systemPrompt },
      ...conversationHistory.map((m) => ({ role: m.role as "user" | "assistant", content: m.content })),
      { role: "user" as const, content: message },
    ];

    // â”€â”€ Ollama primary (local 8B model for election data privacy) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const encoder = new TextEncoder();
    let fullContent = "";
    const ollamaUp = await isOllamaAvailable();

    if (ollamaUp) {
      console.log(`ğŸ¤– [API] LLM backend: Ollama (${OLLAMA_MODEL})`);
      const ollamaResult = await ollamaStream(chatMessages, {
        maxTokens: 1024,
        temperature: 0.0,
      });

      if (ollamaResult) {
        const stream = new ReadableStream({
          async start(controller) {
            try {
              const reader = ollamaResult.getReader();
              while (true) {
                const { done, value } = await reader.read();
                if (done) break;
                if (value) {
                  fullContent += value;
                  controller.enqueue(encoder.encode(`data: ${JSON.stringify({ delta: value })}\n\n`));
                }
              }
              const sourceMatch = fullContent.match(/ğŸ“„ Source:\s*(.+)$/m);
              const source = sourceMatch?.[1]?.trim() ?? "";
              const finalSourceMeta = sourceMatch ? sourceMeta : [];
              setCachedResponse(message, fullContent, source, finalSourceMeta);
              logInteraction({ userType: "poll_worker", question: message, response: fullContent, sourceDoc: source || "N/A", language, cached: false });
              controller.enqueue(encoder.encode(`data: ${JSON.stringify({ source, cached: false, done: true, sourceMeta: finalSourceMeta, usedSidecar: true })}\n\n`));
              controller.close();
            } catch (err) {
              controller.error(err);
            }
          },
        });

        return new Response(stream, {
          headers: { "Content-Type": "text/event-stream", "Cache-Control": "no-cache" },
        });
      }
      console.warn("âš ï¸ [API] Ollama stream returned null â€” falling back to Groq");
    }

    // â”€â”€ Groq fallback (cloud â€” only if Ollama is unavailable) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    console.log(`ğŸ¤– [API] LLM backend: Groq fallback (${GROQ_MODEL})`);
    console.log("ğŸ”‘ [API] GROQ_API_KEY exists:", !!process.env.GROQ_API_KEY);
    const groq = getGroqClient();
    const groqStream = await groq.chat.completions.create({
      model: GROQ_MODEL,
      messages: chatMessages,
      temperature: 0.0,
      max_tokens: 1024,
      stream: true,
    });

    const stream = new ReadableStream({
      async start(controller) {
        try {
          for await (const chunk of groqStream) {
            const delta = chunk.choices[0]?.delta?.content ?? "";
            if (delta) {
              fullContent += delta;
              controller.enqueue(encoder.encode(`data: ${JSON.stringify({ delta })}\n\n`));
            }
          }
          const sourceMatch = fullContent.match(/ğŸ“„ Source:\s*(.+)$/m);
          const source = sourceMatch?.[1]?.trim() ?? "";
          const finalSourceMeta = sourceMatch ? sourceMeta : [];
          setCachedResponse(message, fullContent, source, finalSourceMeta);
          logInteraction({ userType: "poll_worker", question: message, response: fullContent, sourceDoc: source || "N/A", language, cached: false });
          controller.enqueue(encoder.encode(`data: ${JSON.stringify({ source, cached: false, done: true, sourceMeta: finalSourceMeta, usedSidecar: false })}\n\n`));
          controller.close();
        } catch (err) {
          controller.error(err);
        }
      },
    });

    return new Response(stream, {
      headers: { "Content-Type": "text/event-stream", "Cache-Control": "no-cache" },
    });
  } catch (error) {
    console.error("âŒ [API] Fatal error in chat route:", error);
    return NextResponse.json(
      { error: error instanceof Error ? error.message : "Internal server error" },
      { status: 500 }
    );
  }
}
